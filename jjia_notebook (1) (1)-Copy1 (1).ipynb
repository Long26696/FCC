{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a17a29-f1ed-415f-a7ae-cfc518c8e316",
   "metadata": {},
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "def rebin(data, newbinsize):\n",
    "    output = []\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        s = 0\n",
    "        for _ in range(newbinsize):\n",
    "            if i < len(data):\n",
    "                s += data[i]\n",
    "                i += 1\n",
    "        output.append(s)\n",
    "    return output\n",
    "\n",
    "def extract_idx(fname):\n",
    "    \"\"\"Parse 'extracted_data_XX.json' and return the integer XX.\"\"\"\n",
    "    match = re.search(r\"extracted_data_(\\d+)\\.json\", fname)\n",
    "    return int(match.group(1))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) Collect and sort JSON files numerically\n",
    "# --------------------------------------------------------------------\n",
    "json_files = glob.glob(\"extracted_data_*.json\")\n",
    "json_files.sort(key=extract_idx)  # use numeric sort\n",
    "print(\"Processing JSON files in numeric order:\", json_files)\n",
    "\n",
    "# Global settings\n",
    "systematic_value = 0.10\n",
    "rebinfactor = 10\n",
    "\n",
    "# (Loop over each file)\n",
    "for idx, json_file in enumerate(json_files):\n",
    "    print(f\"\\n=== Processing {json_file} (idx={idx}) ===\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    signal_counts = data_dict[\"hist_photon_energy_signal\"][\"entries\"]\n",
    "    background_counts = data_dict[\"hist_photon_energy_BG\"][\"entries\"]\n",
    "\n",
    "    signal_counts = rebin(signal_counts, rebinfactor)\n",
    "    background_counts = rebin(background_counts, rebinfactor)\n",
    "\n",
    "    signal_plus_background = [0.10 * s + b for s, b in zip(signal_counts, background_counts)]\n",
    "    background_uncertainty = [np.sqrt(b) + systematic_value * b for b in background_counts]\n",
    "\n",
    "    # Optional: plot the rebinned S and B\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(signal_counts, label=\"Signal (rebinned)\", alpha=0.7)\n",
    "    plt.plot(background_counts, label=\"Background (rebinned)\", alpha=0.7)\n",
    "    plt.xlabel(\"Rebinned Bins\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(f\"Rebinned S and B for {json_file}\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Rebinned_SB_{idx}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Build pyhf model and observations\n",
    "    model = pyhf.simplemodels.uncorrelated_background(\n",
    "        signal=signal_counts,\n",
    "        bkg=background_counts,\n",
    "        bkg_uncertainty=background_uncertainty,\n",
    "    )\n",
    "    observations = signal_plus_background + model.config.auxdata\n",
    "\n",
    "    # Dynamic POI range helper:\n",
    "    def dynamic_poi_range(obs_limit, exp_limit, factor=1.5, npoints=10):\n",
    "        poi_max = factor * max(obs_limit, exp_limit)\n",
    "        return np.linspace(0.0, poi_max, npoints)\n",
    "\n",
    "    # Quick upper-limit scan with a default range (e.g., 0-6)\n",
    "    default_poi_values = np.linspace(0, 6, 10)\n",
    "    obs_lim_temp, exp_limits_temp, (scan_temp, results_temp) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        observations, model, default_poi_values, level=0.05, return_results=True\n",
    "    )\n",
    "\n",
    "    poi_values = dynamic_poi_range(obs_lim_temp, exp_limits_temp[2], factor=1.5, npoints=10)\n",
    "    obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        observations, model, poi_values, level=0.05, return_results=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "    print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "    # Brazil band plot:\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.set_title(f\"Brazil band for {json_file}\")\n",
    "    brazil.plot_results(poi_values, results, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"BrazilBand_{idx}.png\")\n",
    "    plt.close()\n",
    "    print(f\"Created Brazil band plot: BrazilBand_{idx}.png\")\n",
    "\n",
    "print(\"\\nAll files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d87cd32-f2f3-42b0-b9f5-3eeaa25290c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     11\u001b[0m json_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_data_*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mjson_files\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# numeric sort\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebin\u001b[39m(data, newbinsize):\n\u001b[1;32m     15\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m, in \u001b[0;36mparse_index\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_index\u001b[39m(fname):\n\u001b[1;32m      8\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_data_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import glob, json, re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "def parse_index(fname):\n",
    "    match = re.search(r\"extracted_data_(\\d+)\\.json\", fname)\n",
    "    return int(match.group(1))\n",
    "\n",
    "json_files = glob.glob(\"extracted_data_*.json\")\n",
    "json_files.sort(key=parse_index)  # numeric sort\n",
    "\n",
    "def rebin(data, newbinsize):\n",
    "    output = []\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        s = 0\n",
    "        for _ in range(newbinsize):\n",
    "            if i < len(data):\n",
    "                s += data[i]\n",
    "                i += 1\n",
    "        output.append(s)\n",
    "    return output\n",
    "\n",
    "for idx, json_file in enumerate(json_files):\n",
    "    print(f\"\\n=== Processing file index {idx}, name={json_file} ===\")\n",
    "\n",
    "    # 1) Read the JSON\n",
    "    with open(json_file,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    signal_counts = data[\"hist_photon_energy_signal\"][\"entries\"]\n",
    "    background_counts = data[\"hist_photon_energy_BG\"][\"entries\"]\n",
    "\n",
    "\n",
    "    # 2) Rebin\n",
    "    signal_counts = rebin(signal_counts, 10)\n",
    "    background_counts = rebin(background_counts, 10)\n",
    "\n",
    "     # Debug check\n",
    "    print(\"  signal[:5] =\", signal_counts[:5], \"sum=\", sum(signal_counts))\n",
    "    print(\"  background[:5] =\", background_counts[:5], \"sum=\", sum(background_counts))\n",
    "\n",
    "    # 3) S+B\n",
    "    signal_plus_bkg = [0.1*s + b for s,b in zip(signal_counts, background_counts)]\n",
    "    bkg_unc = [np.sqrt(b) + 0.1*b for b in background_counts]\n",
    "\n",
    "    # 4) Build model & fit\n",
    "    model = pyhf.simplemodels.uncorrelated_background(signal=signal_counts,\n",
    "                                                      bkg=background_counts,\n",
    "                                                      bkg_uncertainty=bkg_unc)\n",
    "    observations = signal_plus_bkg + model.config.auxdata\n",
    "\n",
    "    # Quick fit\n",
    "    fit_result = pyhf.infer.mle.fit(observations, model)\n",
    "    print(\"  fit result =\", fit_result)\n",
    "\n",
    "    # 5) Quick upper-limit\n",
    "    poi_vals = np.linspace(0, 10, 10)\n",
    "    obs_lim, exp_lims, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        observations, model, poi_vals, level=0.05, return_results=True\n",
    "    )\n",
    "    print(f\"  obs limit = {obs_lim:.3f}, exp limit central= {exp_lims[2]:.3f}\")\n",
    "\n",
    "    # 6) Plot\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.set_title(f\"Brazil band for {json_file}\")\n",
    "    brazil.plot_results(poi_vals, results, ax=ax)\n",
    "\n",
    "    max_limit = max(obs_lim, max(exp_lims))\n",
    "\n",
    "\n",
    "    xmax = max(6, 1.2 * max_limit)\n",
    "\n",
    "\n",
    "    ax.set_xlim(0, xmax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f\"BrazilBand_{idx}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa15b8a-7743-4934-8e01-28ed5f581475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f33c22-cc5d-4ab2-80d1-cef5ad45af8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948db58f-47a8-48df-b260-525b07e123f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f1172-f733-44eb-b666-1e972538e1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050720ea-7454-4602-b301-69bd2aa2956a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b91830ff-8ad0-4716-919c-07b94235f7cc",
   "metadata": {},
   "source": [
    "# Pseudocode for saving raw data from a TTree:\n",
    "import json\n",
    "import ROOT\n",
    "\n",
    "infile = ROOT.TFile(\"test_Higgsino_100_50_simple.root\")\n",
    "tree = infile.Get(\"outputTree\")\n",
    "\n",
    "photon_energies_signal = []\n",
    "for event in tree:\n",
    "    for phE in event.ph_E:  # or however your variable is named\n",
    "        photon_energies_signal.append(phE)\n",
    "\n",
    "# Save it to JSON\n",
    "with open(\"raw_data.json\", \"w\") as f:\n",
    "    json.dump({\"photonE_signal\": photon_energies_signal}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eea3c1-6e28-49d0-9886-b60d2f2cb3d3",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_input_data(\n",
    "    signal_data,\n",
    "    background_data,\n",
    "    variable_name=\"Photon Energy\",\n",
    "    bin_size=None,\n",
    "    nbins=None,\n",
    "    output_name=\"input_plot.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the raw data for signal and background, letting matplotlib do the binning.\n",
    "    \n",
    "    Arguments:\n",
    "        signal_data      : list or np.array of signal values\n",
    "        background_data  : list or np.array of background values\n",
    "        variable_name    : string for x-axis label\n",
    "        bin_size         : if given, we'll build bin edges of that width\n",
    "        nbins            : if given (and bin_size is None), we let matplotlib do that many bins\n",
    "        output_name      : name of the output image file\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    if bin_size:\n",
    "        # We'll figure out the range from both datasets\n",
    "        data_min = min(min(signal_data), min(background_data))\n",
    "        data_max = max(max(signal_data), max(background_data))\n",
    "        bin_edges = np.arange(data_min, data_max + bin_size, bin_size)\n",
    "        plt.hist(signal_data, bins=bin_edges, alpha=0.5, label=\"Signal\")\n",
    "        plt.hist(background_data, bins=bin_edges, alpha=0.5, label=\"Background\")\n",
    "    else:\n",
    "        # If user did not provide bin_size, we can default to nbins or 50\n",
    "        if not nbins:\n",
    "            nbins = 50\n",
    "        plt.hist(signal_data, bins=nbins, alpha=0.5, label=\"Signal\")\n",
    "        plt.hist(background_data, bins=nbins, alpha=0.5, label=\"Background\")\n",
    "\n",
    "    plt.xlabel(variable_name)\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Signal vs Background\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis=\"y\", alpha=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_name)\n",
    "    plt.close()\n",
    "    print(f\"Saved input plot to {output_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13f524-8fda-4d95-be87-3cee1aa09aa9",
   "metadata": {},
   "source": [
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "def rebin(data, newbinsize):\n",
    "    \"\"\"\n",
    "    Combine `newbinsize` consecutive bins from `data` into a single bin.\n",
    "    E.g., rebinfactor=10 merges groups of 10 bins into 1 bin.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    origcount = 0\n",
    "    while origcount < len(data):\n",
    "        newbincontent = 0\n",
    "        for _ in range(newbinsize):\n",
    "            if origcount < len(data):\n",
    "                newbincontent += data[origcount]\n",
    "                origcount += 1\n",
    "            else:\n",
    "                break\n",
    "        output.append(newbincontent)\n",
    "    return output\n",
    "\n",
    "def plot_fit_inputs(signal_counts, background_counts, background_uncertainty, rebinfactor, outname):\n",
    "    \"\"\"\n",
    "    Make a plot of rebinned signal vs. background with error bars \n",
    "    (showing the sqrt(bkg) + systematic).\n",
    "    \"\"\"\n",
    "    # We'll treat each bin index as the \"center\" of that bin\n",
    "    xvals = np.arange(len(signal_counts))\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    \n",
    "    # Plot background as a marker with error bars\n",
    "    plt.errorbar(\n",
    "        xvals, background_counts, \n",
    "        yerr=background_uncertainty, \n",
    "        fmt='o', color='blue', label='Background'\n",
    "    )\n",
    "    \n",
    "    # Plot signal as a step or line\n",
    "    plt.step(\n",
    "        xvals, signal_counts, \n",
    "        where='mid', color='red', label='Signal'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(f\"Rebinned bins (factor={rebinfactor})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Input distributions to the fit\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outname)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Find all the extracted_data JSON files\n",
    "# --------------------------------------------------------------------\n",
    "json_files = glob.glob(\"extracted_data_*.json\")\n",
    "json_files.sort()  # sort them so they go in numerical order\n",
    "\n",
    "# We'll keep a 10% systematic\n",
    "systematic_value = 0.10\n",
    "\n",
    "# Rebin factor\n",
    "rebinfactor = 10\n",
    "\n",
    "for idx, json_file in enumerate(json_files):\n",
    "    print(f\"\\n=== Processing {json_file} ===\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # 2. Load data from the JSON file\n",
    "    # ----------------------------------------------------------------\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    # These keys must match how you stored them in extracted_data_X.json\n",
    "    signal_counts = data_dict[\"hist_photon_energy_signal\"][\"entries\"]\n",
    "    background_counts = data_dict[\"hist_photon_energy_BG\"][\"entries\"]\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3. Rebin signal + background data\n",
    "    # ----------------------------------------------------------------\n",
    "    signal_counts = rebin(signal_counts, rebinfactor)\n",
    "    background_counts = rebin(background_counts, rebinfactor)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4. Define background uncertainties: sqrt(N_bkg) + 10% * N_bkg\n",
    "    # ----------------------------------------------------------------\n",
    "    background_uncertainty = [np.sqrt(b) + systematic_value * b for b in background_counts]\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 5. Plot the input distributions\n",
    "    # ----------------------------------------------------------------\n",
    "    input_plot_name = f\"input_hist_{idx}.png\"\n",
    "    plot_fit_inputs(\n",
    "        signal_counts, \n",
    "        background_counts, \n",
    "        background_uncertainty, \n",
    "        rebinfactor, \n",
    "        outname=input_plot_name\n",
    "    )\n",
    "    print(f\"Created input distribution plot: {input_plot_name}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 6. Build the pyhf model\n",
    "    #    We'll treat 'signal_counts' as the shape of the signal\n",
    "    #    and 'background_counts' as the shape of the background.\n",
    "    #    The background_uncertainty is uncorrelated for each bin.\n",
    "    # ----------------------------------------------------------------\n",
    "    model = pyhf.simplemodels.uncorrelated_background(\n",
    "        signal=signal_counts,\n",
    "        bkg=background_counts,\n",
    "        bkg_uncertainty=background_uncertainty,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 7. Observations = signal_plus_background + auxdata\n",
    "    #    If you want to see a \"signal injection\" scenario at 10% of signal\n",
    "    #    plus background, you can do:\n",
    "    # ----------------------------------------------------------------\n",
    "    signal_plus_background = [\n",
    "        0.10 * s + b for s, b in zip(signal_counts, background_counts)\n",
    "    ]\n",
    "    observations = signal_plus_background + model.config.auxdata\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 8. Fit the model\n",
    "    # ----------------------------------------------------------------\n",
    "    fit_result = pyhf.infer.mle.fit(data=observations, pdf=model)\n",
    "    print(f\"Fit result (POI, nuisance params...): {fit_result}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 9. Hypothesis test: compute CLs for mu=1\n",
    "    # ----------------------------------------------------------------\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "        1.0,  # test mu=1\n",
    "        observations,\n",
    "        model,\n",
    "        test_stat=\"qtilde\",\n",
    "        return_expected_set=True,\n",
    "    )\n",
    "    print(f\"Observed CLs: {CLs_obs:.4f}\")\n",
    "    print(f\"Expected CLs: {[f'{val:.4f}' for val in CLs_exp]}\")\n",
    "    # ----------------------------------------------------------------\n",
    "    # 10. Upper-limit scan\n",
    "    # ----------------------------------------------------------------\n",
    "    poi_values = np.linspace(0.0, 10, 10)\n",
    "    obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        observations, model, poi_values, level=0.05, return_results=True\n",
    "    )\n",
    "    print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "    print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 11. Brazil band plot\n",
    "    # ----------------------------------------------------------------\n",
    "    brazil_fig_name = f\"BrazilBand_{idx}.png\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    ax.set_title(f\"Brazil band for {json_file}\")\n",
    "    brazil.plot_results(poi_values, results, ax=ax)\n",
    "    x_min = poi_values.min() - 1\n",
    "    x_max = poi_values.max() + 1\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(brazil_fig_name)\n",
    "    plt.close()\n",
    "    print(f\"Created Brazil band plot: {brazil_fig_name}\")\n",
    "\n",
    "print(\"\\nAll files processed.\")\n",
    "print(f\"Expected CLs (exp): {[f'{val:.4f}' for val in CLs_exp]}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082685b5-6f9a-4641-a50f-c4bcfb78c332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969c60c-98a5-483b-acc1-30795395b580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f1d1366-2709-4b63-94ab-7e560365b0ae",
   "metadata": {},
   "source": [
    "# for row in fit_results:\n",
    "#     print(f\"idx={row['idx']:2d}, m1={row['m1']}, m2={row['m2']},\"\n",
    "#           f\" obs={row['obs_mu95']:.3f}, exp={row['exp_mu95']:.3f}\")\n",
    "\n",
    "# # -----------------------------------------------------------------------\n",
    "# # 5) Build 2D \"exclusion\" plot\n",
    "# #    We'll define \"excluded\" if obs_mu95 < 1, etc.\n",
    "# # -----------------------------------------------------------------------\n",
    "# m1vals = np.array([r[\"m1\"] for r in fit_results])\n",
    "# m2vals = np.array([r[\"m2\"] for r in fit_results])\n",
    "# obs_vals= np.array([r[\"obs_mu95\"] for r in fit_results])\n",
    "# exp_vals= np.array([r[\"exp_mu95\"] for r in fit_results])\n",
    "\n",
    "# excluded_obs = (obs_vals < 1.0).astype(float)  # 1=excluded, 0=not\n",
    "# excluded_exp = (exp_vals < 1.0).astype(float)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# triang = tri.Triangulation(m1vals, m2vals)\n",
    "\n",
    "# xvals = np.array([r[\"m1\"] for r in fit_results])\n",
    "# yvals = np.array([r[\"m2\"] for r in fit_results])\n",
    "# if len(np.unique(xvals)) < 2 or len(np.unique(yvals)) < 2:\n",
    "#     print(\"Not enough 2D points to form a Triangulation. Using scatter only.\")\n",
    "#     plt.scatter(xvals, yvals, color=\"red\")\n",
    "# else:\n",
    "#     triang = tri.Triangulation(xvals, yvals)\n",
    "#     plt.tricontourf(triang, excluded_obs, levels=[-0.5,0.5,1.5], colors=[\"white\",\"red\"])\n",
    "\n",
    "\n",
    "# # Fill with \"observed\" region => red\n",
    "# plt.tricontourf(triang, excluded_obs,\n",
    "#                 levels=[-0.5,0.5,1.5], # 0 -> 0.5 => not excluded, 0.5->1 => excluded\n",
    "#                 colors=[\"white\",\"red\"], alpha=0.3)\n",
    "# # Observed boundary in black, solid\n",
    "# obs_cont = plt.tricontour(triang, excluded_obs, levels=[0.5],\n",
    "#                           colors=[\"black\"], linestyles=[\"-\"], linewidths=2)\n",
    "\n",
    "# if obs_cont and len(obs_cont.allsegs[0]) > 0:\n",
    "#     obs_cont.collections[0].set_label(\"Observed\")\n",
    "\n",
    "\n",
    "# if exp_cont and len(exp_cont.allsegs[0]) > 0:\n",
    "#     exp_cont.collections[0].set_label(\"Expected\")\n",
    "\n",
    "# # Expected boundary in blue, dashed\n",
    "# exp_cont = plt.tricontour(triang, excluded_exp, levels=[0.5],\n",
    "#                           colors=[\"blue\"], linestyles=[\"--\"], linewidths=2)\n",
    "# exp_cont.collections[0].set_label(\"Expected\")\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# if labels:\n",
    "#     plt.legend(loc=\"best\")\n",
    "# else:\n",
    "#     print(\"No valid contours. Skipping legend.\")\n",
    "\n",
    "\n",
    "# excluded_obs = (obs_vals < 1.0).astype(float)\n",
    "\n",
    "\n",
    "# plt.xlabel(\"$m_{\\\\tilde{\\\\chi}_2^0}$ [GeV]\")\n",
    "# plt.ylabel(\"$m_{\\\\tilde{\\\\chi}_1^0}$ [GeV]\")\n",
    "# plt.title(\"FCC-ee SUSY Exclusion @ 95% CL\")\n",
    "# plt.legend(loc=\"best\")\n",
    "\n",
    "# print(\"Skipping legend due to degenerate contour.\")\n",
    "\n",
    "# plt.grid(True, alpha=0.4)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"SUSY_Exclusion_2D.png\",dpi=150)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nFinal 2D plot saved as 'SUSY_Exclusion_2D.png'\")\n",
    "\n",
    "# # Summaries\n",
    "# for row in fit_results:\n",
    "#     print(f\"idx={row['idx']:2d}, m1={row['m1']}, m2={row['m2']},\"\n",
    "#           f\" obs={row['obs_mu95']:.3f}, exp={row['exp_mu95']:.3f}\")\n",
    "\n",
    "# m1vals = np.array([r[\"m1\"] for r in fit_results])\n",
    "# m2vals = np.array([r[\"m2\"] for r in fit_results])\n",
    "# obs_vals= np.array([r[\"obs_mu95\"] for r in fit_results])\n",
    "# exp_vals= np.array([r[\"exp_mu95\"] for r in fit_results])\n",
    "\n",
    "# excluded_obs = (obs_vals < 1.0).astype(float)  # 1=excluded\n",
    "# excluded_exp = (exp_vals < 1.0).astype(float)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "\n",
    "# # If not enough distinct points for a Triangulation, just do scatter:\n",
    "# if len(np.unique(m1vals))<2 or len(np.unique(m2vals))<2:\n",
    "#     print(\"Not enough 2D points to form a Triangulation. Using scatter only.\")\n",
    "#     plt.scatter(m1vals, m2vals, color=\"red\")\n",
    "# else:\n",
    "#     triang = tri.Triangulation(m1vals, m2vals)\n",
    "\n",
    "#     # Fill the plane with white (excluded=0) or red (excluded=1)\n",
    "#     plt.tricontourf(\n",
    "#         triang, excluded_obs,\n",
    "#         levels=[-0.5,0.5,1.5],\n",
    "#         colors=[\"yellow\",\"red\"],\n",
    "#         alpha=0.3\n",
    "#     )\n",
    "\n",
    "#     # Observed boundary in black\n",
    "#     obs_cont = plt.tricontour(\n",
    "#         triang, excluded_obs,\n",
    "#         levels=[0.5],\n",
    "#         colors=[\"black\"],\n",
    "#         linestyles=[\"-\"],\n",
    "#         linewidths=2\n",
    "#     )\n",
    "#     if obs_cont and len(obs_cont.allsegs[0])>0:\n",
    "#         obs_cont.collections[0].set_label(\"Observed\")\n",
    "\n",
    "#     # Expected boundary in blue\n",
    "#     exp_cont = plt.tricontour(\n",
    "#         triang, excluded_exp,\n",
    "#         levels=[0.5],\n",
    "#         colors=[\"blue\"],\n",
    "#         linestyles=[\"--\"],\n",
    "#         linewidths=2\n",
    "#     )\n",
    "#     # Check if it's empty\n",
    "#     if exp_cont and len(exp_cont.allsegs[0])>0:\n",
    "#         exp_cont.collections[0].set_label(\"Expected\")\n",
    "\n",
    "#     # Build legend only if we have some valid labels\n",
    "#     handles, labels = plt.gca().get_legend_handles_labels()\n",
    "#     if labels:\n",
    "#         plt.legend(loc=\"best\")\n",
    "#     else:\n",
    "#         print(\"No valid contours. Skipping legend.\")\n",
    "\n",
    "# plt.xlabel(\"$m_{\\\\tilde{\\\\chi}_2^0}$ [GeV]\")\n",
    "# plt.ylabel(\"$m_{\\\\tilde{\\\\chi}_1^0}$ [GeV]\")\n",
    "# plt.title(\"FCC-ee SUSY Exclusion @ 95% CL\")\n",
    "# plt.grid(True, alpha=0.4)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"SUSY_Exclusion_2D.png\", dpi=150)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nFinal 2D plot saved as 'SUSY_Exclusion_2D.png'\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "# Suppose fit_results is already defined, each element like:\n",
    "# {\n",
    "#   \"idx\": ...,\n",
    "#   \"m1\": (heavier mass, e.g. chi2),\n",
    "#   \"m2\": (lighter mass, e.g. chi1),\n",
    "#   \"obs_mu95\": ...,\n",
    "#   \"exp_mu95\": ...\n",
    "# }\n",
    "\n",
    "# 1) Print the results\n",
    "for row in fit_results:\n",
    "    print(f\"idx={row['idx']:2d}, m1={row['m1']}, m2={row['m2']},\"\n",
    "          f\" obs={row['obs_mu95']:.3f}, exp={row['exp_mu95']:.3f}\")\n",
    "\n",
    "# 2) Extract mass arrays and upper limits\n",
    "m1vals = np.array([r[\"m1\"] for r in fit_results])  # e.g. chi2 mass\n",
    "m2vals = np.array([r[\"m2\"] for r in fit_results])  # e.g. chi1 mass\n",
    "\n",
    "obs_vals= np.array([r[\"obs_mu95\"] for r in fit_results])\n",
    "exp_vals= np.array([r[\"exp_mu95\"] for r in fit_results])\n",
    "\n",
    "# 3) Convert them into x,y for plotting\n",
    "#    x-axis = m(chi2)\n",
    "#    y-axis = Δm = m(chi2) - m(chi1)\n",
    "xvals = m1vals\n",
    "yvals = m1vals - m2vals  # the mass splitting\n",
    "\n",
    "# 4) Define excluded vs. allowed\n",
    "excluded_obs = (obs_vals < 1.0).astype(float)  # 1 => excluded\n",
    "excluded_exp = (exp_vals < 1.0).astype(float)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# 5) If not enough distinct points for Triangulation, just do scatter\n",
    "if len(np.unique(xvals)) < 2 or len(np.unique(yvals)) < 2:\n",
    "    print(\"Not enough 2D points to form a Triangulation. Using scatter only.\")\n",
    "    # At least show the points\n",
    "    plt.scatter(xvals, yvals, color=\"red\")\n",
    "else:\n",
    "    # Build triangulation\n",
    "    triang = tri.Triangulation(xvals, yvals)\n",
    "\n",
    "    # (A) Fill the plane with two colors:\n",
    "    #     levels=[-0.5, 0.5, 1.5] => \"allowed\" if <0.5, \"excluded\" if >=0.5\n",
    "    plt.tricontourf(\n",
    "        triang,\n",
    "        excluded_obs,\n",
    "        levels=[-0.5,0.5,1.5],\n",
    "        colors=[\"yellow\",\"red\"],\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    # (B) Observed boundary in black\n",
    "    obs_cont = plt.tricontour(\n",
    "        triang,\n",
    "        excluded_obs,\n",
    "        levels=[0.5],\n",
    "        colors=[\"black\"],\n",
    "        linestyles=[\"-\"],\n",
    "        linewidths=2\n",
    "    )\n",
    "    if obs_cont and len(obs_cont.allsegs[0]) > 0:\n",
    "        obs_cont.collections[0].set_label(\"Observed\")\n",
    "\n",
    "    # (C) Expected boundary in blue\n",
    "    exp_cont = plt.tricontour(\n",
    "        triang,\n",
    "        excluded_exp,\n",
    "        levels=[0.5],\n",
    "        colors=[\"blue\"],\n",
    "        linestyles=[\"--\"],\n",
    "        linewidths=2\n",
    "    )\n",
    "    if exp_cont and len(exp_cont.allsegs[0]) > 0:\n",
    "        exp_cont.collections[0].set_label(\"Expected\")\n",
    "\n",
    "    # Build legend only if we have valid labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    if labels:\n",
    "        plt.legend(loc=\"best\")\n",
    "    else:\n",
    "        print(\"No valid contours. Skipping legend.\")\n",
    "\n",
    "# 6) Axis labels and finishing touches\n",
    "plt.xlabel(r\"$m(\\tilde{\\chi}_2^0)\\text{ [GeV]}$\")\n",
    "plt.ylabel(r\"$\\Delta m = m(\\tilde{\\chi}_2^0)-m(\\tilde{\\chi}_1^0)$ [GeV]\")\n",
    "\n",
    "\n",
    "plt.title(\"FCC-ee SUSY Exclusion @ 95% CL\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SUSY_Exclusion_2D.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal 2D plot saved as 'SUSY_Exclusion_2D.png'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e0bf9-d98d-4e49-b9c6-dfdc46482466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b30d8-ffcd-4e4a-80a4-b89822318ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c60c94-661f-4afe-a17e-8e52ee9a5829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b69199-5552-4157-b87e-106865ca1812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5bac9-864f-419e-b5ad-c802e40aac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ce6c5b-7bc4-4f8f-a615-c620fc165696",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'m_N2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         data_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     13\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m---> 14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm_N2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm_N2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_m\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_m\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan),  \u001b[38;5;66;03m# Ensure these keys exist\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_limits\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_limits\u001b[39m\u001b[38;5;124m\"\u001b[39m, [np\u001b[38;5;241m.\u001b[39mnan]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     18\u001b[0m         })\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Sort data by m_N2\u001b[39;00m\n\u001b[1;32m     21\u001b[0m data_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(data, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm_N2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'm_N2'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect data from all JSON files\n",
    "json_files = glob.glob(\"extracted_data_*.json\")\n",
    "data = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "        data.append({\n",
    "            \"m_N2\": data_dict[\"m_N2\"],\n",
    "            \"delta_m\": data_dict[\"delta_m\"],\n",
    "            \"obs_limit\": data_dict.get(\"obs_limit\", np.nan),  # Ensure these keys exist\n",
    "            \"exp_limits\": data_dict.get(\"exp_limits\", [np.nan]*5)\n",
    "        })\n",
    "\n",
    "# Sort data by m_N2\n",
    "data_sorted = sorted(data, key=lambda x: x[\"m_N2\"])\n",
    "m_N2_values = [d[\"m_N2\"] for d in data_sorted]\n",
    "delta_m_values = [d[\"delta_m\"] for d in data_sorted]\n",
    "obs_limits = [d[\"obs_limit\"] for d in data_sorted]\n",
    "exp_limits = [d[\"exp_limits\"] for d in data_sorted]\n",
    "\n",
    "# Extract expected bands\n",
    "exp_2sigma_low = [e[0] for e in exp_limits]\n",
    "exp_1sigma_low = [e[1] for e in exp_limits]\n",
    "exp_median = [e[2] for e in exp_limits]\n",
    "exp_1sigma_high = [e[3] for e in exp_limits]\n",
    "exp_2sigma_high = [e[4] for e in exp_limits]\n",
    "\n",
    "# Create the exclusion plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Brazil bands\n",
    "plt.fill_between(m_N2_values, exp_2sigma_low, exp_2sigma_high, color='lime', alpha=0.3, label=\"Expected ±2σ\")\n",
    "plt.fill_between(m_N2_values, exp_1sigma_low, exp_1sigma_high, color='yellow', alpha=0.5, label=\"Expected ±1σ\")\n",
    "plt.plot(m_N2_values, exp_median, 'k--', label=\"Expected median\")\n",
    "\n",
    "# Observed limit\n",
    "plt.plot(m_N2_values, obs_limits, 'ro-', markersize=5, label=\"Observed\")\n",
    "\n",
    "# Labels and styling\n",
    "plt.xlabel(r\"$m(\\tilde{\\chi}_2^0)$ [GeV]\", fontsize=12)\n",
    "plt.ylabel(r\"$\\Delta m = m(\\tilde{\\chi}_2^0) - m(\\tilde{\\chi}_1^0)$ [GeV]\", fontsize=12)\n",
    "plt.title(\"Higgsino Exclusion Limits (95% CL)\", fontsize=14)\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.yscale('log')  # Use if limits span orders of magnitude\n",
    "plt.xlim(min(m_N2_values)-10, max(m_N2_values)+10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(\"Higgsino_Exclusion_Limits.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15260dd0-380b-44d5-910b-89b5b77e9a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39716d-6fdc-4356-9443-49dbc851ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c73f5c-65fc-4526-bc24-ec63fdcfe2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing extracted_data_0(1).json ===\n",
      "Created input distribution plot: input_hist_0.png\n",
      "Fit result (POI, nuisance params...): [0.10002243 0.99995972 0.99999856 0.9999988  0.9999988  0.99999872\n",
      " 0.99999863 0.99999785 0.99999861 0.99999875 0.99999961]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phoenix666/.local/lib/python3.8/site-packages/pyhf/infer/calculators.py:418: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  teststat = (qmu - qmu_A) / (2 * self.sqrtqmuA_v)\n",
      "/home/phoenix666/.local/lib/python3.8/site-packages/pyhf/infer/calculators.py:467: RuntimeWarning: invalid value encountered in divide\n",
      "  CLs = tensorlib.astensor(CLsb / CLb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed CLs: nan\n",
      "Expected CLs: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
      "Upper limit (obs): μ = nan\n",
      "Upper limit (exp): μ = 2.1667\n",
      "Created Brazil band plot: BrazilBand_0.png\n",
      "\n",
      "=== Processing extracted_data_0.json ===\n",
      "Created input distribution plot: input_hist_1.png\n",
      "Fit result (POI, nuisance params...): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Observed CLs: 0.5000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_optimize.py:353: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper limit (obs): μ = 2.1111\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_1.png\n",
      "\n",
      "=== Processing extracted_data_1(1).json ===\n",
      "Created input distribution plot: input_hist_2.png\n",
      "Fit result (POI, nuisance params...): [0.09985837 1.00011586 1.00000578 1.00000591 1.0000061  1.00000875\n",
      " 1.00000928 1.00000959 1.00001201 1.0000093  1.00000038]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Upper limit (obs): μ = 1.0556\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_2.png\n",
      "\n",
      "=== Processing extracted_data_1.json ===\n",
      "Created input distribution plot: input_hist_3.png\n",
      "Fit result (POI, nuisance params...): [0.09985837 1.00011586 1.00000578 1.00000591 1.0000061  1.00000875\n",
      " 1.00000928 1.00000959 1.00001201 1.0000093  1.00000038]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Upper limit (obs): μ = 1.0556\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_3.png\n",
      "\n",
      "=== Processing extracted_data_10.json ===\n",
      "Created input distribution plot: input_hist_4.png\n",
      "Fit result (POI, nuisance params...): [0.07924498 1.0007646  1.00073376 1.00078294 1.0008456  1.00089974\n",
      " 1.00104893 1.00065852 1.00002134 1.00001841 1.00004688]\n",
      "Observed CLs: 0.3265\n",
      "Expected CLs: ['0.0535', '0.1331', '0.3025', '0.5795', '0.8531']\n",
      "Upper limit (obs): μ = 2.1062\n",
      "Upper limit (exp): μ = 2.0768\n",
      "Created Brazil band plot: BrazilBand_4.png\n",
      "\n",
      "=== Processing extracted_data_11.json ===\n",
      "Created input distribution plot: input_hist_5.png\n",
      "Fit result (POI, nuisance params...): [0.09951791 1.0005647  1.00002292 1.00001355 1.00002173 1.00002634\n",
      " 1.00002333 1.0000178  1.00001422 1.0000133  1.00000434]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Upper limit (obs): μ = 1.0556\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_5.png\n",
      "\n",
      "=== Processing extracted_data_12.json ===\n",
      "Created input distribution plot: input_hist_6.png\n",
      "Fit result (POI, nuisance params...): [0.10373835 0.99899012 0.99988515 0.99988914 0.99988541 0.99990655\n",
      " 0.9999663  1.00000527 1.00000831 0.99998862 1.00000821]\n",
      "Observed CLs: 0.0068\n",
      "Expected CLs: ['0.0000', '0.0003', '0.0035', '0.0328', '0.1835']\n",
      "Upper limit (obs): μ = 1.0579\n",
      "Upper limit (exp): μ = 1.0566\n",
      "Created Brazil band plot: BrazilBand_6.png\n",
      "\n",
      "=== Processing extracted_data_13.json ===\n",
      "Created input distribution plot: input_hist_7.png\n",
      "Fit result (POI, nuisance params...): [0.10583371 0.99929817 0.99983186 0.99982779 0.99983302 0.99985397\n",
      " 0.99997216 1.00000369 0.99998995 1.00001291 1.00002266]\n",
      "Observed CLs: 0.2069\n",
      "Expected CLs: ['0.0187', '0.0615', '0.1817', '0.4381', '0.7642']\n",
      "Upper limit (obs): μ = 1.8824\n",
      "Upper limit (exp): μ = 1.8236\n",
      "Created Brazil band plot: BrazilBand_7.png\n",
      "\n",
      "=== Processing extracted_data_14.json ===\n",
      "Created input distribution plot: input_hist_8.png\n",
      "Fit result (POI, nuisance params...): [0.125701   0.99849304 0.9992327  0.99923319 0.99929072 0.99943388\n",
      " 0.99996698 1.00000162 0.99999872 0.99999861 1.0000016 ]\n",
      "Observed CLs: 0.4462\n",
      "Expected CLs: ['0.1142', '0.2291', '0.4268', '0.6910', '0.9066']\n",
      "Upper limit (obs): μ = 2.6817\n",
      "Upper limit (exp): μ = 2.5901\n",
      "Created Brazil band plot: BrazilBand_8.png\n",
      "\n",
      "=== Processing extracted_data_15.json ===\n",
      "Created input distribution plot: input_hist_9.png\n",
      "Fit result (POI, nuisance params...): [0.08872673 1.00035058 1.00032818 1.00033129 1.00031969 1.00024744\n",
      " 1.00000415 0.99999745 1.00001094 0.99999087 0.99999602]\n",
      "Observed CLs: 0.5481\n",
      "Expected CLs: ['0.1919', '0.3300', '0.5337', '0.7692', '0.9372']\n",
      "Upper limit (obs): μ = 3.2297\n",
      "Upper limit (exp): μ = 3.1914\n",
      "Created Brazil band plot: BrazilBand_9.png\n",
      "\n",
      "=== Processing extracted_data_16.json ===\n",
      "Created input distribution plot: input_hist_10.png\n",
      "Fit result (POI, nuisance params...): [0.09678813 1.00009617 1.00009609 1.00009018 1.00009147 1.00006547\n",
      " 1.00000039 1.00000043 1.0000016  0.99999963 0.99999883]\n",
      "Observed CLs: 0.5582\n",
      "Expected CLs: ['0.2011', '0.3410', '0.5443', '0.7762', '0.9397']\n",
      "Upper limit (obs): μ = 3.2754\n",
      "Upper limit (exp): μ = 3.2389\n",
      "Created Brazil band plot: BrazilBand_10.png\n",
      "\n",
      "=== Processing extracted_data_17.json ===\n",
      "Created input distribution plot: input_hist_11.png\n",
      "Fit result (POI, nuisance params...): [0.09948628 1.00035515 1.00001763 1.0000159  1.0000164  1.00001478\n",
      " 1.00000861 1.00000763 1.00000156 0.99999969 1.00000028]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Upper limit (obs): μ = nan\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_11.png\n",
      "\n",
      "=== Processing extracted_data_18.json ===\n",
      "Created input distribution plot: input_hist_12.png\n",
      "Fit result (POI, nuisance params...): [0.09873633 1.00038114 1.00002834 1.00002034 1.0000111  1.00001187\n",
      " 1.00000333 0.99999962 0.99999677 0.99999855 0.99999636]\n",
      "Observed CLs: 0.0028\n",
      "Expected CLs: ['0.0000', '0.0001', '0.0013', '0.0154', '0.1126']\n",
      "Upper limit (obs): μ = 1.0563\n",
      "Upper limit (exp): μ = 1.0558\n",
      "Created Brazil band plot: BrazilBand_12.png\n",
      "\n",
      "=== Processing extracted_data_19.json ===\n",
      "Created input distribution plot: input_hist_13.png\n",
      "Fit result (POI, nuisance params...): [0.13276834 0.99665205 0.99937023 0.99953794 0.9998955  1.00000714\n",
      " 1.         1.00009069 1.00004806 1.0000479  0.99984066]\n",
      "Observed CLs: 0.3129\n",
      "Expected CLs: ['0.0483', '0.1236', '0.2883', '0.5649', '0.8451']\n",
      "Upper limit (obs): μ = 2.0693\n",
      "Upper limit (exp): μ = 2.0398\n",
      "Created Brazil band plot: BrazilBand_13.png\n",
      "\n",
      "=== Processing extracted_data_2.json ===\n",
      "Created input distribution plot: input_hist_14.png\n",
      "Fit result (POI, nuisance params...): [0.09850724 1.00058871 1.00006318 1.00006328 1.00007265 1.00008789\n",
      " 1.00010099 1.00010491 1.00012735 1.0000548  1.00000713]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0002', '0.0046']\n",
      "Upper limit (obs): μ = 1.0556\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_14.png\n",
      "\n",
      "=== Processing extracted_data_20.json ===\n",
      "Created input distribution plot: input_hist_15.png\n",
      "Fit result (POI, nuisance params...): [0.16891332 0.99672273 0.99869879 0.99917393 0.99991245 0.99999953\n",
      " 1.         0.99999523 0.9999964  1.00002658 1.00000742]\n",
      "Observed CLs: 0.6151\n",
      "Expected CLs: ['0.2585', '0.4056', '0.6036', '0.8138', '0.9523']\n",
      "Upper limit (obs): μ = 3.8437\n",
      "Upper limit (exp): μ = 3.7718\n",
      "Created Brazil band plot: BrazilBand_15.png\n",
      "\n",
      "=== Processing extracted_data_21.json ===\n",
      "Created input distribution plot: input_hist_16.png\n",
      "Fit result (POI, nuisance params...): [0.05782477 1.0011636  1.00078909 1.00045708 1.00000584 0.99999978\n",
      " 1.         0.99999975 1.00000029 1.00000039 1.00000072]\n",
      "Observed CLs: 0.7363\n",
      "Expected CLs: ['0.4187', '0.5634', '0.7305', '0.8842', '0.9732']\n",
      "Upper limit (obs): μ = 5.5455\n",
      "Upper limit (exp): μ = 5.5024\n",
      "Created Brazil band plot: BrazilBand_16.png\n",
      "\n",
      "=== Processing extracted_data_22.json ===\n",
      "Created input distribution plot: input_hist_17.png\n",
      "Fit result (POI, nuisance params...): [0.36859828 0.99479439 0.99506053 0.99731005 1.         0.99999996\n",
      " 1.         0.99999807 1.00000092 0.99999457 0.99999896]\n",
      "Observed CLs: 0.7880\n",
      "Expected CLs: ['0.5030', '0.6372', '0.7831', '0.9101', '0.9800']\n",
      "Upper limit (obs): μ = 6.8523\n",
      "Upper limit (exp): μ = 6.7710\n",
      "Created Brazil band plot: BrazilBand_17.png\n",
      "\n",
      "=== Processing extracted_data_23.json ===\n",
      "Created input distribution plot: input_hist_18.png\n",
      "Fit result (POI, nuisance params...): [0.07927992 1.00039535 1.00037567 1.00020438 1.         1.00000013\n",
      " 1.         0.99999913 0.99999895 0.99999922 0.99999996]\n",
      "Observed CLs: 0.7978\n",
      "Expected CLs: ['0.5264', '0.6569', '0.7965', '0.9164', '0.9816']\n",
      "Upper limit (obs): μ = 6.9806\n",
      "Upper limit (exp): μ = 6.9137\n",
      "Created Brazil band plot: BrazilBand_18.png\n",
      "\n",
      "=== Processing extracted_data_24.json ===\n",
      "Created input distribution plot: input_hist_19.png\n",
      "Fit result (POI, nuisance params...): [0.10128025 0.99992943 0.9999908  0.99999325 0.99999938 0.99999861\n",
      " 1.         0.9999984  0.99999612 1.00000132 0.99999811]\n",
      "Observed CLs: 0.6009\n",
      "Expected CLs: ['0.2436', '0.3893', '0.5891', '0.8049', '0.9494']\n",
      "Upper limit (obs): μ = 3.6372\n",
      "Upper limit (exp): μ = 3.5482\n",
      "Created Brazil band plot: BrazilBand_19.png\n",
      "\n",
      "=== Processing extracted_data_25.json ===\n",
      "Created input distribution plot: input_hist_20.png\n",
      "Fit result (POI, nuisance params...): [0.8945548  0.99469576 0.99870925 0.99999878 1.         0.99999991\n",
      " 1.         1.00000018 0.99999977 0.99999986 1.00000016]\n",
      "Observed CLs: nan\n",
      "Expected CLs: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
      "Upper limit (obs): μ = 10.0000\n",
      "Upper limit (exp): μ = 10.0000\n",
      "Created Brazil band plot: BrazilBand_20.png\n",
      "\n",
      "=== Processing extracted_data_26.json ===\n",
      "Created input distribution plot: input_hist_21.png\n",
      "Fit result (POI, nuisance params...): [0.99942263 0.99867954 1.00000001 0.99999999 1.         1.00000032\n",
      " 1.         1.00000001 1.00000662 1.00000265 0.9999984 ]\n",
      "Observed CLs: 0.9996\n",
      "Expected CLs: ['0.9988', '0.9993', '0.9996', '0.9999', '1.0000']\n",
      "Upper limit (obs): μ = 10.0000\n",
      "Upper limit (exp): μ = 10.0000\n",
      "Created Brazil band plot: BrazilBand_21.png\n",
      "\n",
      "=== Processing extracted_data_27.json ===\n",
      "Created input distribution plot: input_hist_22.png\n",
      "Fit result (POI, nuisance params...): [0.13200636 0.99846739 0.99980639 0.9998474  0.99999015 0.99999997\n",
      " 1.         0.99999838 1.00000006 1.00000211 1.00000234]\n",
      "Observed CLs: 0.6426\n",
      "Expected CLs: ['0.2905', '0.4394', '0.6327', '0.8310', '0.9578']\n",
      "Upper limit (obs): μ = 4.1034\n",
      "Upper limit (exp): μ = 4.0514\n",
      "Created Brazil band plot: BrazilBand_22.png\n",
      "\n",
      "=== Processing extracted_data_28.json ===\n",
      "Created input distribution plot: input_hist_23.png\n",
      "Fit result (POI, nuisance params...): [1.00381307 0.99571532 0.99933694 1.00000027 1.         1.00000213\n",
      " 1.         0.99999025 1.0000026  0.99999941 1.00001015]\n",
      "Observed CLs: 1.0000\n",
      "Expected CLs: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
      "Upper limit (obs): μ = 10.0000\n",
      "Upper limit (exp): μ = 10.0000\n",
      "Created Brazil band plot: BrazilBand_23.png\n",
      "\n",
      "=== Processing extracted_data_29.json ===\n",
      "Created input distribution plot: input_hist_24.png\n",
      "Fit result (POI, nuisance params...): [1.00000124 0.99995033 1.         1.         1.         0.99999969\n",
      " 1.         1.         1.00000048 0.99999993 0.99999999]\n",
      "Observed CLs: 1.0000\n",
      "Expected CLs: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
      "Upper limit (obs): μ = 10.0000\n",
      "Upper limit (exp): μ = 10.0000\n",
      "Created Brazil band plot: BrazilBand_24.png\n",
      "\n",
      "=== Processing extracted_data_3.json ===\n",
      "Created input distribution plot: input_hist_25.png\n",
      "Fit result (POI, nuisance params...): [0.09962962 1.00006913 1.00001639 1.00001683 1.00001655 1.00002065\n",
      " 1.00002518 1.00002387 1.00002783 1.00001251 1.00000045]\n",
      "Observed CLs: 0.0220\n",
      "Expected CLs: ['0.0002', '0.0017', '0.0137', '0.0850', '0.3285']\n",
      "Upper limit (obs): μ = 1.0663\n",
      "Upper limit (exp): μ = 1.0618\n",
      "Created Brazil band plot: BrazilBand_25.png\n",
      "\n",
      "=== Processing extracted_data_4.json ===\n",
      "Created input distribution plot: input_hist_26.png\n",
      "Fit result (POI, nuisance params...): [0.09871909 1.00011952 1.00005327 1.00005698 1.00005872 1.00007123\n",
      " 1.00008584 1.00009139 1.00009418 1.00004298 1.00000011]\n",
      "Observed CLs: 0.0914\n",
      "Expected CLs: ['0.0031', '0.0160', '0.0715', '0.2509', '0.5917']\n",
      "Upper limit (obs): μ = 1.2799\n",
      "Upper limit (exp): μ = 1.1048\n",
      "Created Brazil band plot: BrazilBand_26.png\n",
      "\n",
      "=== Processing extracted_data_5.json ===\n",
      "Created input distribution plot: input_hist_27.png\n",
      "Fit result (POI, nuisance params...): [0.10309319 0.99985969 0.99987146 0.99986983 0.99985399 0.99983276\n",
      " 0.99978841 0.99978329 0.99976696 0.9999088  1.00000213]\n",
      "Observed CLs: 0.1272\n",
      "Expected CLs: ['0.0064', '0.0274', '0.1043', '0.3165', '0.6615']\n",
      "Upper limit (obs): μ = 1.5937\n",
      "Upper limit (exp): μ = 1.4349\n",
      "Created Brazil band plot: BrazilBand_27.png\n",
      "\n",
      "=== Processing extracted_data_6.json ===\n",
      "Created input distribution plot: input_hist_28.png\n",
      "Fit result (POI, nuisance params...): [0.09946478 1.00026545 1.00002061 1.00002045 1.00002155 1.00002478\n",
      " 1.00002721 1.00001953 1.00000646 1.00000033 1.00000043]\n",
      "Observed CLs: 0.0000\n",
      "Expected CLs: ['0.0000', '0.0000', '0.0000', '0.0000', '0.0001']\n",
      "Upper limit (obs): μ = 1.0556\n",
      "Upper limit (exp): μ = 1.0556\n",
      "Created Brazil band plot: BrazilBand_28.png\n",
      "\n",
      "=== Processing extracted_data_7.json ===\n",
      "Created input distribution plot: input_hist_29.png\n",
      "Fit result (POI, nuisance params...): [0.10092528 0.99978402 0.99996767 0.99996433 0.99995821 0.99996154\n",
      " 0.99995205 0.99997438 0.99999382 0.99998983 1.00001553]\n",
      "Observed CLs: 0.0146\n",
      "Expected CLs: ['0.0001', '0.0009', '0.0086', '0.0614', '0.2708']\n",
      "Upper limit (obs): μ = 1.0618\n",
      "Upper limit (exp): μ = 1.0589\n",
      "Created Brazil band plot: BrazilBand_29.png\n",
      "\n",
      "=== Processing extracted_data_8.json ===\n",
      "Created input distribution plot: input_hist_30.png\n",
      "Fit result (POI, nuisance params...): [0.09196312 1.00089811 1.0002955  1.00030286 1.00032084 1.00034832\n",
      " 1.00037428 1.00026785 1.00001095 0.99999618 0.99999857]\n",
      "Observed CLs: 0.1630\n",
      "Expected CLs: ['0.0110', '0.0413', '0.1387', '0.3749', '0.7145']\n",
      "Upper limit (obs): μ = 1.7593\n",
      "Upper limit (exp): μ = 1.6636\n",
      "Created Brazil band plot: BrazilBand_30.png\n",
      "\n",
      "=== Processing extracted_data_9.json ===\n",
      "Created input distribution plot: input_hist_31.png\n",
      "Fit result (POI, nuisance params...): [0.10404048 0.99979775 0.99985723 0.99984497 0.99983497 0.99982991\n",
      " 0.99980284 0.9998726  1.00000058 0.99999869 0.99999665]\n",
      "Observed CLs: 0.2988\n",
      "Expected CLs: ['0.0435', '0.1144', '0.2741', '0.5500', '0.8367']\n",
      "Upper limit (obs): μ = 2.0604\n",
      "Upper limit (exp): μ = 2.0282\n",
      "Created Brazil band plot: BrazilBand_31.png\n",
      "\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "def rebin(data, newbinsize):\n",
    "    \"\"\"\n",
    "    Combine newbinsize consecutive bins from data into a single bin.\n",
    "    E.g., rebinfactor=10 merges groups of 10 bins into 1 bin.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    origcount = 0\n",
    "    while origcount < len(data):\n",
    "        newbincontent = 0\n",
    "        for _ in range(newbinsize):\n",
    "            if origcount < len(data):\n",
    "                newbincontent += data[origcount]\n",
    "                origcount += 1\n",
    "            else:\n",
    "                break\n",
    "        output.append(newbincontent)\n",
    "    return output\n",
    "\n",
    "def plot_fit_inputs(signal_counts, background_counts, background_uncertainty, rebinfactor, outname):\n",
    "    \"\"\"\n",
    "    Make a plot of rebinned signal vs. background with error bars \n",
    "    (showing the sqrt(bkg) + systematic).\n",
    "    \"\"\"\n",
    "    # We'll treat each bin index as the \"center\" of that bin\n",
    "    xvals = np.arange(len(signal_counts))\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    \n",
    "    # Plot background as a marker with error bars\n",
    "    plt.errorbar(\n",
    "        xvals, background_counts, \n",
    "        yerr=background_uncertainty, \n",
    "        fmt='o', color='blue', label='Background'\n",
    "    )\n",
    "    \n",
    "    # Plot signal as a step or line\n",
    "    plt.step(\n",
    "        xvals, signal_counts, \n",
    "        where='mid', color='red', label='Signal'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(f\"Rebinned bins (factor={rebinfactor})\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(\"Input distributions to the fit\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outname)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Find all the extracted_data JSON files\n",
    "# --------------------------------------------------------------------\n",
    "json_files = glob.glob(\"extracted_data_*.json\")\n",
    "json_files.sort()  # sort them so they go in numerical order\n",
    "\n",
    "# We'll keep a 10% systematic\n",
    "systematic_value = 0.10\n",
    "\n",
    "# Rebin factor\n",
    "rebinfactor = 10\n",
    "\n",
    "for idx, json_file in enumerate(json_files):\n",
    "    print(f\"\\n=== Processing {json_file} ===\")\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # 2. Load data from the JSON file\n",
    "    # ----------------------------------------------------------------\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    # These keys must match how you stored them in extracted_data_X.json\n",
    "    signal_counts = data_dict[\"hist_photon_energy_signal\"][\"entries\"]\n",
    "    background_counts = data_dict[\"hist_photon_energy_BG\"][\"entries\"]\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3. Rebin signal + background data\n",
    "    # ----------------------------------------------------------------\n",
    "    signal_counts = rebin(signal_counts, rebinfactor)\n",
    "    background_counts = rebin(background_counts, rebinfactor)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 4. Define background uncertainties: sqrt(N_bkg) + 10% * N_bkg\n",
    "    # ----------------------------------------------------------------\n",
    "    background_uncertainty = [np.sqrt(b) + systematic_value * b for b in background_counts]\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 5. Plot the input distributions\n",
    "    # ----------------------------------------------------------------\n",
    "    input_plot_name = f\"input_hist_{idx}.png\"\n",
    "    plot_fit_inputs(\n",
    "        signal_counts, \n",
    "        background_counts, \n",
    "        background_uncertainty, \n",
    "        rebinfactor, \n",
    "        outname=input_plot_name\n",
    "    )\n",
    "    print(f\"Created input distribution plot: {input_plot_name}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 6. Build the pyhf model\n",
    "    #    We'll treat 'signal_counts' as the shape of the signal\n",
    "    #    and 'background_counts' as the shape of the background.\n",
    "    #    The background_uncertainty is uncorrelated for each bin.\n",
    "    # ----------------------------------------------------------------\n",
    "    model = pyhf.simplemodels.uncorrelated_background(\n",
    "        signal=signal_counts,\n",
    "        bkg=background_counts,\n",
    "        bkg_uncertainty=background_uncertainty,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 7. Observations = signal_plus_background + auxdata\n",
    "    #    If you want to see a \"signal injection\" scenario at 10% of signal\n",
    "    #    plus background, you can do:\n",
    "    # ----------------------------------------------------------------\n",
    "    signal_plus_background = [\n",
    "        0.10 * s + b for s, b in zip(signal_counts, background_counts)\n",
    "    ]\n",
    "    observations = signal_plus_background + model.config.auxdata\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 8. Fit the model\n",
    "    # ----------------------------------------------------------------\n",
    "    fit_result = pyhf.infer.mle.fit(data=observations, pdf=model)\n",
    "    print(f\"Fit result (POI, nuisance params...): {fit_result}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 9. Hypothesis test: compute CLs for mu=1\n",
    "    # ----------------------------------------------------------------\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "        1.0,  # test mu=1\n",
    "        observations,\n",
    "        model,\n",
    "        test_stat=\"qtilde\",\n",
    "        return_expected_set=True,\n",
    "    )\n",
    "    print(f\"Observed CLs: {CLs_obs:.4f}\")\n",
    "    print(f\"Expected CLs: {[f'{val:.4f}' for val in CLs_exp]}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 10. Upper-limit scan\n",
    "    # ----------------------------------------------------------------\n",
    "    poi_values = np.linspace(0.0, 10, 10)\n",
    "    obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        observations, model, poi_values, level=0.05, return_results=True\n",
    "    )\n",
    "    print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "    print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 11. Brazil band plot\n",
    "    # ----------------------------------------------------------------\n",
    "    brazil_fig_name = f\"BrazilBand_{idx}.png\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_title(f\"Brazil band for {json_file}\")\n",
    "    brazil.plot_results(poi_values, results, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(brazil_fig_name)\n",
    "    plt.close()\n",
    "    print(f\"Created Brazil band plot: {brazil_fig_name}\")\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7aa8f5-353e-47de-8aba-4a7119dda63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
